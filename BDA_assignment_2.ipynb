{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7pyeshwanth/bda-assignment-2/blob/main/BDA_assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Big Data Analytics Assignment**"
      ],
      "metadata": {
        "id": "JC8UxzF2lG4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIUwrcw3CI3T",
        "outputId": "e7776bc0-17b3-4ab7-dcc1-59364477059e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***ðŸ’¼ Classification Model using Logistic Regression in PySpark***"
      ],
      "metadata": {
        "id": "SU8qbRL_w6ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Required Libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Step 2: Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Breast Cancer Classification\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 3: Load Breast Cancer Dataset from sklearn\n",
        "breast_cancer_data = load_breast_cancer()\n",
        "pandas_df = pd.DataFrame(breast_cancer_data.data, columns=breast_cancer_data.feature_names)\n",
        "pandas_df['target'] = breast_cancer_data.target\n",
        "\n",
        "# Step 4: Convert Pandas DataFrame to Spark DataFrame\n",
        "spark_df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "# Step 5: Feature Engineering - Assemble Feature Columns\n",
        "feature_columns = [col for col in spark_df.columns if col != 'target']\n",
        "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Step 6: Initialize Logistic Regression Model\n",
        "logistic_regression = LogisticRegression(featuresCol=\"features\", labelCol=\"target\")\n",
        "\n",
        "# Step 7: Build Pipeline for Transformation and Modeling\n",
        "pipeline = Pipeline(stages=[vector_assembler, logistic_regression])\n",
        "\n",
        "# Step 8: Train the Model\n",
        "model = pipeline.fit(spark_df)\n",
        "\n",
        "# Step 9: Generate Predictions on Training Data\n",
        "predictions = model.transform(spark_df)\n",
        "predictions.select(\"features\", \"target\", \"prediction\").show(5, truncate=False)\n",
        "\n",
        "# Step 10: Evaluate Model Performance\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"target\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc_score = evaluator.evaluate(predictions)\n",
        "print(f\"Area Under ROC Curve (AUC): {auc_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9fVUoLPtudD",
        "outputId": "95f8ae3e-80d9-467e-d2c5-429cbbe4647c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+----------+\n",
            "|features                                                                                                                                                                                                            |target|prediction|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+----------+\n",
            "|[17.99,10.38,122.8,1001.0,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871,1.095,0.9053,8.589,153.4,0.006399,0.04904,0.05373,0.01587,0.03003,0.006193,25.38,17.33,184.6,2019.0,0.1622,0.6656,0.7119,0.2654,0.4601,0.1189] |0     |0.0       |\n",
            "|[20.57,17.77,132.9,1326.0,0.08474,0.07864,0.0869,0.07017,0.1812,0.05667,0.5435,0.7339,3.398,74.08,0.005225,0.01308,0.0186,0.0134,0.01389,0.003532,24.99,23.41,158.8,1956.0,0.1238,0.1866,0.2416,0.186,0.275,0.08902]|0     |0.0       |\n",
            "|[19.69,21.25,130.0,1203.0,0.1096,0.1599,0.1974,0.1279,0.2069,0.05999,0.7456,0.7869,4.585,94.03,0.00615,0.04006,0.03832,0.02058,0.0225,0.004571,23.57,25.53,152.5,1709.0,0.1444,0.4245,0.4504,0.243,0.3613,0.08758]  |0     |0.0       |\n",
            "|[11.42,20.38,77.58,386.1,0.1425,0.2839,0.2414,0.1052,0.2597,0.09744,0.4956,1.156,3.445,27.23,0.00911,0.07458,0.05661,0.01867,0.05963,0.009208,14.91,26.5,98.87,567.7,0.2098,0.8663,0.6869,0.2575,0.6638,0.173]      |0     |0.0       |\n",
            "|[20.29,14.34,135.1,1297.0,0.1003,0.1328,0.198,0.1043,0.1809,0.05883,0.7572,0.7813,5.438,94.44,0.01149,0.02461,0.05688,0.01885,0.01756,0.005115,22.54,16.67,152.2,1575.0,0.1374,0.205,0.4,0.1625,0.2364,0.07678]     |0     |0.0       |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Area Under ROC Curve (AUC): 0.9996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***KMeans Clustering on Iris Dataset using PySpark***"
      ],
      "metadata": {
        "id": "ekdSvP3X06CY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the Iris dataset from sklearn\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['label'] = iris.target  # Add target labels for reference\n",
        "\n",
        "# Step 2: Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Iris KMeans Clustering\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Convert the Pandas DataFrame to a Spark DataFrame\n",
        "spark_df = spark.createDataFrame(df)\n",
        "\n",
        "# Show the first few rows of the dataset\n",
        "spark_df.show(5)\n",
        "\n",
        "# Step 3: Feature Engineering using VectorAssembler\n",
        "feature_columns = [col for col in df.columns if col != 'label']  # Exclude the target column 'label'\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Show the transformed dataframe with features vector\n",
        "assembled_df = assembler.transform(spark_df)\n",
        "assembled_df.select(\"label\", \"features\").show(5)\n",
        "\n",
        "# Step 4: Apply KMeans Clustering\n",
        "kmeans = KMeans(k=3, seed=1, featuresCol=\"features\", predictionCol=\"prediction\")\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[assembler, kmeans])\n",
        "\n",
        "# Fit the model\n",
        "model = pipeline.fit(spark_df)\n",
        "\n",
        "# Step 5: Make Predictions\n",
        "predictions = model.transform(spark_df)\n",
        "\n",
        "# Show the first 5 rows with cluster predictions\n",
        "predictions.select(\"label\", \"prediction\").show(5)\n",
        "\n",
        "# Step 6: Evaluate Clustering Model (Corrected)\n",
        "wssse = model.stages[-1].summary.trainingCost  # Accessing the WSSSE from the model summary\n",
        "print(f\"Within Set Sum of Squared Errors (WSSSE): {wssse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhNp1H44y-wA",
        "outputId": "21d09dd3-4012-4932-f140-46f3b7b54132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------------+-----------------+----------------+-----+\n",
            "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|label|\n",
            "+-----------------+----------------+-----------------+----------------+-----+\n",
            "|              5.1|             3.5|              1.4|             0.2|    0|\n",
            "|              4.9|             3.0|              1.4|             0.2|    0|\n",
            "|              4.7|             3.2|              1.3|             0.2|    0|\n",
            "|              4.6|             3.1|              1.5|             0.2|    0|\n",
            "|              5.0|             3.6|              1.4|             0.2|    0|\n",
            "+-----------------+----------------+-----------------+----------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----+-----------------+\n",
            "|label|         features|\n",
            "+-----+-----------------+\n",
            "|    0|[5.1,3.5,1.4,0.2]|\n",
            "|    0|[4.9,3.0,1.4,0.2]|\n",
            "|    0|[4.7,3.2,1.3,0.2]|\n",
            "|    0|[4.6,3.1,1.5,0.2]|\n",
            "|    0|[5.0,3.6,1.4,0.2]|\n",
            "+-----+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|    0|         1|\n",
            "|    0|         1|\n",
            "|    0|         1|\n",
            "|    0|         1|\n",
            "|    0|         1|\n",
            "+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Within Set Sum of Squared Errors (WSSSE): 78.85566582597731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***ðŸ“š Book Recommendation with PySpark***"
      ],
      "metadata": {
        "id": "K4fj2d3R_qvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import pandas as pd\n",
        "\n",
        "# Step 2: Initialize a Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Book Recommendation Engine\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 3: Load the ratings data from the Goodbooks-10k dataset\n",
        "# The dataset contains user-book ratings with columns: user_id, book_id, rating\n",
        "url = \"https://raw.githubusercontent.com/zygmuntz/goodbooks-10k/master/ratings.csv\"\n",
        "df_pd = pd.read_csv(url)\n",
        "\n",
        "# Select relevant columns for recommendation system\n",
        "df_pd = df_pd[['user_id', 'book_id', 'rating']]\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "df = spark.createDataFrame(df_pd)\n",
        "\n",
        "# Display sample data\n",
        "print(\"Sample Book Ratings:\")\n",
        "df.show(5)\n",
        "\n",
        "# Step 4: Split data into training and testing sets\n",
        "training_data, testing_data = df.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Step 5: Build ALS (Alternating Least Squares) recommendation model\n",
        "als_model = ALS(\n",
        "    userCol=\"user_id\",\n",
        "    itemCol=\"book_id\",\n",
        "    ratingCol=\"rating\",\n",
        "    coldStartStrategy=\"drop\"  # Avoids NaN predictions\n",
        ")\n",
        "\n",
        "# Train the model on the training dataset\n",
        "model = als_model.fit(training_data)\n",
        "\n",
        "# Step 6: Generate predictions on test dataset\n",
        "predictions = model.transform(testing_data)\n",
        "\n",
        "# Step 7: Evaluate the model using Root Mean Squared Error (RMSE)\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\",\n",
        "    labelCol=\"rating\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse:.4f}\")\n",
        "\n",
        "# Step 8: Generate Top-5 book recommendations for\n",
        "recommendations = model.recommendForAllUsers(5)\n",
        "\n",
        "# Display sample recommendations\n",
        "print(\"Top-5 Movie Recommendations for Users:\")\n",
        "recommendations.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIu5kx3v8_9L",
        "outputId": "79f00231-d8de-4d65-9db5-c151136d4161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Book Ratings:\n",
            "+-------+-------+------+\n",
            "|user_id|book_id|rating|\n",
            "+-------+-------+------+\n",
            "|      1|    258|     5|\n",
            "|      2|   4081|     4|\n",
            "|      2|    260|     5|\n",
            "|      2|   9296|     5|\n",
            "|      2|   2318|     3|\n",
            "+-------+-------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Root Mean Squared Error (RMSE) on test data: 0.8214\n",
            "Top-5 Movie Recommendations for Users:\n",
            "+-------+----------------------------------------------------------------------------------------------+\n",
            "|user_id|recommendations                                                                               |\n",
            "+-------+----------------------------------------------------------------------------------------------+\n",
            "|1      |[{9566, 4.537521}, {3628, 4.52148}, {7254, 4.4257407}, {5580, 4.415051}, {6920, 4.411584}]    |\n",
            "|3      |[{1338, 2.5264645}, {4154, 2.4865828}, {2236, 2.4503016}, {9347, 2.3921611}, {464, 2.3821602}]|\n",
            "|5      |[{3628, 4.945353}, {6590, 4.8637643}, {9566, 4.8148603}, {5580, 4.799803}, {4483, 4.7857976}] |\n",
            "|6      |[{8946, 5.1851897}, {9842, 4.9534235}, {3491, 4.950713}, {3628, 4.9334836}, {6920, 4.889859}] |\n",
            "|9      |[{8946, 4.3468084}, {3953, 4.2792153}, {9024, 4.228249}, {6920, 4.2187743}, {7593, 4.207723}] |\n",
            "+-------+----------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}